Method1 :
Using Kubeadm:

There are two scripts under kubernetes/installation/kubeadm:
  - common.sh  : run on all nodes
  - master.sh  : run on only master node


download the shell script and give the permission as below and run:
Login to all the nodes (master and worker) and run common.sh:
chmod +x common.sh
sh common.sh

once you run on all the nodes run below on only master node:
chmod +x master.sh
sh master.sh

Copy the `kubeadm join` command and run on all the worker nodes with sudo:
kubeadm join IpAddressOfMaster:6443 --token $token --discovery-token-ca-cert-hash sha256:$hash


### you will find a kubeadm join command here in the output which you can run on any machine make that as a worker node ###
###   kubeadm join <control-plane-host>:<control-plane-port> --token <token> --discovery-token-ca-cert-hash sha256:<hash>  ###

### If you do not have the token, you can get it by running the following command on the control-plane node: ###
### kubeadm token list ###
### By default, tokens expire after 24 hours. If you are joining a node to the cluster after the current token has expired,
### you can create a new token by running the following command on the control-plane node: ###
### kubeadm token create ###

### if you don't have the value of --discovery-token-ca-cert-hash, you can get it by running the following command chain on the control-plane node:
### openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'


or you can generate a new command with below:
sudo kubeadm token create --print-join-command


By Checking API status we will know if our cluster is working fine or not:

kubectl get --raw='/readyz?verbose'


Method 2:
Installation using kind.
There is a script under kubernetes/installation/kind :
 - kind-cluster.sh

Download that and do the following:

chmod +x kind-cluster.sh
sh kind-cluster.sh



Once Installation is done check the nodes:

kubectl get nodes

We can see control-plane and worker nodes. We can even update the labels:
kubectl label node <node-name> node-role.kubernetes.io/worker=worker1



kind: kind of resources
apiVersion: supported version of api (v1, apps/v1)
metadata: An object containing:
name: Required if generateName is not specified. The name of this pod. It must be an RFC1035 compatible value and be unique within the namespace.
labels: Optional. Labels are arbitrary key:value pairs that can be used by Deployment and services for grouping and targeting pods.
generateName: Required if name is not set. A prefix to use to generate a unique name. Has the same validation rules as name.
namespace: Required. The namespace of the pod.
annotations: Optional. A map of string keys and values that can be used by external tooling to store and retrieve arbitrary metadata about objects.
spec: The pod specification. See The spec schema for details.



Check the Namespace:
kubectl get namespace

Get all the reousrces in Namespace:
kubectl get all -n kube-system

Create a Namespace:
kubectl create namespace devops

Run a application:
kubectl run my-nginx --image=nginx --replicas=2 --port=80
kubectl get pods
kubectl get deployments

kubectl expose deployment my-nginx --port=80 --type=LoadBalancer
kubectl get services


kubectl delete deployment my-nginx





Deployment:

Step 1:  Create a file, nginx.yaml and enter the following code

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
 replicas: 3
 selector:
  matchLabels:
    app: nginx
 template:
  metadata:
    labels:
      app: nginx
  spec:
   containers:
     - name: nginx
       image: nginx:1.7.9
       ports:
         - containerPort: 80


Create the deployment using the following command:
kubectl create -f nginx.yaml -n $namespace


Verify the procedure, by listing the pods:
kubectl get pod


Describe the resource
kubectl describe deployment $deploymentname
kubectl describe pod $podname


Verify the replica counts:
kubectl get replicaset
kubectl describe replicaset $replicasetname


Create a Yaml from resources:
kubectl get deployment $deploymentname
kubectl get deployment $deploymentname -o yaml
kubectl get deployment $deploymentname -o yaml >> deployment.yaml


check logs for pod:
kubectl logs $podname

if multiple container within the pod:
kubectl logs $podname --container $yourcontainername


check logs with labels:
kubectl logs -l app=nginx

check logs with selector:
kubectl logs --selector app=nginx



Check events:
kubectl get events



Multi-containers:

multicontainer.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
 #replicas: 3
  containers:
    - name: container1
      image: ubuntu
      command: ["/bin/bash", "-c",
       "while true; do echo Hello-Coder; sleep 5 ; done"]
    - name : container2
      image: ubuntu
      command: ["/bin/bash', "-c" ,
      "while true; do echo Hello-Programmer; sleep 5 ; done"]




Service:
kubectl create service nodeport <name-of-service> --tcp=<port-of-service>:<port-of-container>

kubectl create service nodeport nginx --tcp 80:80

kubectl get svc

kubectl describe svc nginx

Our application must be accessible now:
IPaddress:NodePort



Ingress:

Check the supported version here:
https://github.com/kubernetes/ingress-nginx


kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.9.3/deploy/static/provider/cloud/deploy.yaml

kubectl delete -A ValidatingWebhookConfiguration ingress-nginx-admission


ingress.yaml

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
    - host: hello-world.info
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: nginx-service
                port:
                  number: 80


kubectl create â€“f ingress.yaml

kubectl get ingress



Kubernetes metrics server:
helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
helm upgrade --install metrics-server metrics-server/metrics-server -n kube-system


Kubernetes Dashboard:

Using Helm Chart:
https://artifacthub.io/packages/helm/k8s-dashboard/kubernetes-dashboard


helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard


Get the Kubernetes Dashboard URL by running:
export POD_NAME=$(kubectl get pods -n kubernetes-dashboard -l "app.kubernetes.io/name=kubernetes-dashboard,app.kubernetes.io/instance=kubernetes-dashboard" -o jsonpath="{.items[0].metadata.name}")
echo https://127.0.0.1:8443/
kubectl -n kubernetes-dashboard port-forward $POD_NAME 8443:8443


Generate the Token to access dashbaord:
Create SA and ClusterRolebinding  using kubernetes/manifests/nginx/k8s-dashboard-account.yaml

kubectl create -f k8s-dashboard-account.yaml

kubectl -n kubernetes-dashboard create token admin-user



Accessing cluster from outside:
mkdir .kube
cd .kube/
vi config

copy paste the config from master

kubectl config get-context


Learn helm:

https://helm.sh/docs/


Prometheus:

kubectl create ns monitoring
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm install prometheus prometheus-community/kube-prometheus-stack -n monitoring

Expose with port-forward:
kubectl port-forward prometheus-prometheus-kube-prometheus-prometheus-0 9090:9090 -n monitoring

Basic Query:
kube_pod_info{namespace="kube-system"}
kube_pod_container_resource_limits
sum(kube_pod_container_resource_limits{unit="byte"})
sum(kube_pod_info{namespace="kube-system"})

Grafana:
kubectl port-forward prometheus-grafana-68755c9885-k4ghl 8080:3000 -n monitoring

default credential:
user: admin
password: prom-operator


Extra:
Storage:
  - storageclass
  - Persistent volumeMounts
  - persistent volumeclaim

Environment Properties:
  - configmap

Secret:
  - secret

Authentication and permissions:
  - RBAC (Role based access control)
  - clusterrole
  - clusterrolebinding
  - Role
  - RoleBinding
  - Serviceaccount

AutoScaling:
  - hpa (Horizontal Pod Autoscaling) : Commonly used
  - vpa (Vertical Pod AutoScaling)
      - Tools:
        - Karpenter
        - KEDA

AutoSchedule Job:
  - Cronjob
  - Job

All deployment type:
  - Deployment
  - Statefulset
  - daemonset

Multi-tenancy:
  - Network Policies

Health check:
  - Probes:
    - LivenessProbe
    - ReadinessProbe
    - Statrtup Probes

Resource Control:
  - Resources:
    - limits
    - requests
  - ReosourceQuotas


Port in Services:
 - NodePort
 - ClusterIP
 - LoadBalancer







